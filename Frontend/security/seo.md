# SEO - Search Engine Optimization - Поисковая Оптимизация

https://selectel.ru/blog/how-to-improve-seo/?utm_source=youtube.com&utm_medium=referral&utm_campaign=help_seo_290124_UlbiTV_paid

Это оптимизация сайта для повышения его позиции в поисковой выдаче по запросам пользователей.

## Роль семантики в SEO

Поисковые роботы, которые будут парсить нашу страницу, обязательно будут учитывать ее семантику.

## Open Graph и красивые ссылки

Open Graph, или OG, — протокол разметки, который позволяет делать ссылки привлекательными и понятными. Придумали это разработчики из Facebook, а сейчас поддержка Open Graph имеется практически во всех соцсетях.

Open Graph напрямую не влияет на поисковую выдачу, оно улучшает отображение ссылок в соцсетях и дает пользователю дополнительную информацию о странице, на которую ведет линк.

```html

<meta property="og:type" content="website">
<meta property="og:url" content="https://ya.ru">
<meta property="og:title" content="Яндекс поисковик">
<meta property="og:description" content="Найдется все">
<meta property="og:image"
      content="https://habrastorage.org/getpro/geektimes/post_images/368/48c/f98/36848cf9897f462db234b7df4e6e8b42.jpg">
```

## Работа со ссылками в контексте SEO

Представим, что у нас есть две ссылки, которые ведут на одну и ту же страницу: исходный прямой url и тот, что сформировался при работе с категориями или фильтрами. Получается, у нас есть дубликат ссылки — это плохо. По сути, это мусорный контент, который тратит краулинговый бюджет и конкурирует с исходным линком при индексации поисковиками.

Чтобы этого избежать, указываем каноническую ссылку — индексироваться будет только она, а остальные перестанут восприниматься как дубликаты.

```html

<link data-vue-meta="ssr" href="https://habr.com/ru/companies/selectel/articles/788694/" rel="canonical"
      data-vmid="canonical">
```

## Другие важные особенности SEO

### Контроль поисковых движков

#### Метатег robots

При индексации поисковые роботы действуют по определенным алгоритмам, которые нас не всегда устраивают. Мы можем запретить роботам некоторые действия.

Контролирование индексации страницы. `"robots"` - это поисковик, который мы настраиваем.

- Будет индекстироватся `<meta name="robots" content="index">`
- Не будет индекстироватся `<meta name="robots" content="noindex">`
- Разрешает роботу ходить по ссылкам `<meta name="robots" content="follow">`
- Запрещает роботу ходить по ссылкам `<meta name="robots" content="nofollow">`
- Использовать `index` и `follow` `<meta name="robots" content="all">`
- Использовать `noindex` и `nofollow` `<meta name="robots" content="none">`

Google будет индекстировать нашу страницу в отличие от Яндекс

```html
<meta name="googlebot" content="index">
<meta name="yandex" content="noindex">
```

_Разрешающие директивы имеют приоритет над запрещающими: если не прописывать метатег noindex, страница и ссылки по умолчанию будут индексироваться всеми поисковиками. Метатеги нужны только тогда, когда мы хотим что-то запретить._

#### Файл robots.txt
Этот файл нужен, чтобы разрешить или запретить индексирование разделов или страниц сайта для разных роботов.

В примере запрещаем индекстировать `/contacts` поисковику Google и `/admin/*` для Яндекс

```text
User-agent: Googlebot
disallow: /contacts

User-agent: Yandex
disallow: /admin/*
```

В примере ниже запрещаем всем поисковикам индекстировать `/search/*`, `/special/` и позваляем индекстировать `*.js`, `*.css`, `*.jpg`

```text
User-agent: *

Disallow: /search/*
Disallow: /special/

Allow: *.js
Allow: *.css
Allow: *.jpg

Sitemap: https://selectel.ru/sitemap.xml
Sitemap: https://selectel.ru/blog/sitemap_index.xml
```

#### Sitemap
это XML-документ, в котором указывается актуальная структура сайта со всеми ссылками и датами, когда эти ссылки изменялись. Здесь для каждой страницы можно задать четыре свойства:

- `<loc></loc>` - адрес самой страницы;
- `<lastmod></lastmod>` - дата последнего обновления;
- `<changefreq></changefreq>` - частота изменения страницы;
- `<priority></priority>` - приоритет, чтобы какие-то страницы индексировались лучше других, для каждой их них можно установить приоритет индексации от 0 до 1 с шагом 0,1.

Если вы хотите создать большую древовидную структуру, то внутри базового sitemap можно разместить ссылки на другие файлы sitemap.

```xml
<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
    <url>
        <loc>https://selectel.ru/about/newsroom/news/312312</loc>
        <lastmod>2024-08-19T08:32:21.000Z</lastmod>
        <changefreq>monthly</changefreq>
    </url>
    <sitemap>
        <loc>https://selectel.ru/sitemap-pages.xml</loc>
        <lastmod>2024-09-26T11:52:15.106Z</lastmod>
    </sitemap>
</sitemapindex>
```

**Важно понимать, что sitemap не нужен, если сайт содержит всего пару десятков страниц. Это необходимо только для сайтов с большой сложной структурой ссылок и страниц, когда требуется гарантированно проиндексировать нужные канонические ссылки и исключить из индексации некоторые страницы**

**Огромный XML-файл не заполняется руками. Это делается автоматически: с помощью скрипта или генератора.**

### Микроразметка
Микроразметка с точки зрения пользователя — это дополнительная информация под ссылкой в поисковой выдаче. Здесь могут отображаться цены на товары, расписание сеансов в кино или театре, хлебные крошки и так далее.

Инструкция по микрозаметкам https://yandex.ru/support/webmaster/schema-org/

## Проверка поисковой оптимизации
Есть три наиболее важные метрики с технической точки зрения.

- `LCP (Largest Contentful Paint)` — время, за которое загрузился **самый большой** видимый элемент страницы. Хороший показатель — не более 4 секунд, отличный — не более 2,5 секунд;
- `FID (First Input Delay)` — время до **первой интерактивности**. Другими словами, это время с момента загрузки страницы до появления у пользователя возможности взаимодействовать с ней: вводить текст, нажимать кнопки, переходить по ссылкам и так далее. Если FID укладывается в 0,3 секунды, все хорошо. Если нет, над сайтом стоит поработать;
- `CLS (Cumulative Layout Shift)` — метрика, которая отвечает за визуальную стабильность. Сталкивались с ситуацией, когда хотите нажать на какую-нибудь кнопку, а тут «Бах!» - полетела верстка. Это пример визуально нестабильной страницы, и такого, конечно, быть не должно;

Для измерений метрик и узнать способы оптимизация для нашего сайт есть https://pagespeed.web.dev/ от Google, где помимо наиболее важных метрик проверяются другие - это по сути тоже самое, что и `Lighthouse`

## Краулинговый бюджет
Количество страниц, которые может обойти поисковый робот, ограничено. При этом практически на любом сайте есть страницы, которые вообще не должны индексироваться: админские разделы, дубликаты и прочее. Их стоит исключить из индексации, чтобы повысить другие страницы сайта в ранжировании поисковой выдачи.

Важно понимать, что заниматься этой работой стоит тогда, когда вы боретесь за десятые и сотые доли процента SEO. Это актуально, например, для маркетплейсов-гигантов.